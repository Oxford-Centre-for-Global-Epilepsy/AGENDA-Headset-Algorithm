# =======================
# üèÜ Main Workflow Entry
#
#   Workflow Purpose: Preprocess the EEG data files in preparation for algorithm training
# ========================================================================
configfile: "config/config.yaml"

# Snakemake Rules to include
include: "rules/data_preprocessing/bandpass_filter_data.smk"
include: "rules/data_preprocessing/resample_data.smk"
include: "rules/data_preprocessing/convert_to_bipolar_montage.smk"
include: "rules/data_preprocessing/epoch_data.smk"
include: "rules/data_preprocessing/normalise_data_epochs.smk"
include: "rules/data_preprocessing/convert_to_hdf5.smk"

# Define the order of the rules
ruleorder: bandpass_filter_data > resample_data > convert_to_bipolar > epoch_data > normalise_epoched_data > convert_to_hdf5

# ‚úÖ Import `expand` for rule all
#from snakemake.utils import expand
import os
import yaml

# üî• Get $DATA environment variable (ensure it's set)
DATA_PATH = os.getenv("DATA")
if not DATA_PATH:
    raise ValueError("ERROR: The $DATA environment variable is not set!")

data_edf = f"{DATA_PATH}/AGENDA-Headset-Algorithm/data/edf"
data_fif = f"{DATA_PATH}/AGENDA-Headset-Algorithm/data/fif"
data_hdf5 = f"{DATA_PATH}/AGENDA-Headset-Algorithm/data/hdf5"
data_preprocessed = f"{DATA_PATH}/AGENDA-Headset-Algorithm/data/preprocessed"

# ‚úÖ Load Configuration from YAML (for site-specific and montage-specific data processing)
with open("config/config.yaml", "r") as file:
    config = yaml.safe_load(file)

# Load which site data should be processed
SITES_TO_PROCESS = config.get("sites_to_process", [])  # Default to empty list if not set

# Load which headset montage should be used for data processing
MONTAGES = config.get("montages_to_process", [])  # Default to list with 19_channel if not set

# Make a wildcard constraint for the available montages
wildcard_constraints:
    montage="19_channel|17_channel_no_A1_A2|17_channel_no_T3_T4|16_channel"

# =================================================
# üîé Discover Raw Data EEG Files (in .edf format)
# =================================================

def discover_eeg_files(data_folder, file_extension=".EEG", sites=[]):
    """Find all .EEG files in raw data directory, optionally filtering by sites."""
    eeg_files = []
    
    for root, _, files in os.walk(data_folder):
        
        # Extract the site name (first folder in path)
        relative_path = os.path.relpath(root, data_folder)
        site_name = relative_path.split(os.sep)[0]  # Get top-level folder name

        # Only include files from selected sites
        if sites and site_name not in sites:
            continue

        for file in files:
            if file.endswith(file_extension):
                relative_file_path = os.path.relpath(os.path.join(root, file), data_folder)
                eeg_files.append(relative_file_path.replace(file_extension, ""))

    return eeg_files

# üèÜ ** Get list of the file paths to the EEG data (in .edf format) based on configured sites **
EDF_FILEPATHS = discover_eeg_files(data_folder=data_edf, file_extension=".edf", sites=SITES_TO_PROCESS)

print(f"‚úÖ Processing EEG Files from Sites: {SITES_TO_PROCESS}")
print(f"üß† Found {len(EDF_FILEPATHS)} EEG files for processing.")

# =======================
# üèÅ Define rule all
# =======================
rule all:
    input:
        # Ensure all HDF5 files are created
        #expand(data_hdf5 + "/{montage}/{sample}.h5", sample=EDF_FILEPATHS, montage=MONTAGES),
        expand(data_preprocessed + "/{montage}/{sample}.h5", sample=EDF_FILEPATHS, montage=MONTAGES),
        
        # Ensure all tensors are processed
        #expand(data_hdf5 + "/{montage}/{sample}_tensor.done", sample=EDF_FILEPATHS, montage=MONTAGES)


#expand(data_fif + "/{montage}/{sample}_resampled.fif", sample=EDF_FILEPATHS, montage=MONTAGES)
#expand(data_fif + "/{montage}/{sample}_bipolar.fif", sample=EDF_FILEPATHS, montage=MONTAGES)
#expand(data_fif + "/{montage}/{sample}_epoched.fif", sample=EDF_FILEPATHS, montage=MONTAGES)
#expand(data_fif + "/{montage}/{sample}_normalised.fif", sample=EDF_FILEPATHS, montage=MONTAGES)
#expand(data_hdf5 + "/{montage}/{sample}.h5", sample=EDF_FILEPATHS, montage=MONTAGES)
